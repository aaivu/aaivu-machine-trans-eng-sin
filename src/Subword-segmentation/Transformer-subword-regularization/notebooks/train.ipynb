{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1ultNYvvISu"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyYztzhPvOAa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P_QlKLlTMTy"
      },
      "source": [
        "import os\n",
        "os.chdir(\"your directory\")\n",
        "%ls "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3WW1rxWvOHQ"
      },
      "source": [
        "import os\n",
        "os.chdir('path to clone fairseq')\n",
        "#ONE TIME run\n",
        "\n",
        "#clone the  fairseq repository \n",
        "#fairseq - https://github.com/pytorch/fairseq.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYWlX1Bgvxot"
      },
      "source": [
        "#Install fairseq\n",
        "import os\n",
        "os.chdir(\"path to fairseq repository\")\n",
        "!pip install --editable .\n",
        "!pip install sentencepiece sacrebleu tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw4ORJqTwRXA"
      },
      "source": [
        "# Preprocess the data\n",
        "os.chdir('............../scripts')\n",
        "\n",
        "# If joined vocab \n",
        "!bash preprocess-joined-ensi.sh\n",
        "\n",
        "# If independent vocab\n",
        "!bash preprocess-independent-ensi.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N491kgG5DwIc"
      },
      "source": [
        "## **Epoch 200**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoBRSS-tDyLU"
      },
      "source": [
        "# Train the baseline transformer model\n",
        "# For independent vocabulary, update the training command as, \"fairseq-train data-bin/en_si_unigram --source-lang en --target-lang si\"\n",
        "import os\n",
        "os.chdir(\"your directory\")\n",
        "!CUDA_VISIBLE_DEVICES=0 fairseq-train \\\n",
        "    data-bin/en_si_unigram5000/ \\\n",
        "    --source-lang en --target-lang si \\\n",
        "    --arch transformer --share-all-embeddings \\\n",
        "    --encoder-layers 5 --decoder-layers 5 \\\n",
        "    --encoder-embed-dim 512 --decoder-embed-dim 512 \\\n",
        "    --encoder-ffn-embed-dim 2048 --decoder-ffn-embed-dim 2048 \\\n",
        "    --encoder-attention-heads 2 --decoder-attention-heads 2 \\\n",
        "    --encoder-normalize-before --decoder-normalize-before \\\n",
        "    --dropout 0.3 --attention-dropout 0.2 --relu-dropout 0.2 \\\n",
        "    --weight-decay 0.0001 \\\n",
        "    --label-smoothing 0.2 --criterion label_smoothed_cross_entropy \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0 \\\n",
        "    --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-7 \\\n",
        "    --lr 1e-3 --min-lr 1e-9 \\\n",
        "    --batch-size 32 \\\n",
        "    --update-freq 4 \\\n",
        "    --max-epoch 200 --save-interval 10 \\\n",
        "    --tensorboard-logdir logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g6N23dOD2ck"
      },
      "source": [
        "#200 epoch - validation set\n",
        "# Obtain the BLEU score for validation dataset\n",
        "# For independent vocabulary, update the scoring command as, \"fairseq-generate data-bin/en_si_unigram --source-lang en --target-lang si\"\n",
        "os.chdir(\"your directory\")\n",
        "!fairseq-generate \\\n",
        "    data-bin/en_si_unigram5000/ \\\n",
        "    --source-lang en --target-lang si \\\n",
        "    --path checkpoints/checkpoint_best.pt \\\n",
        "    --beam 5 --lenpen 1.2 \\\n",
        "    --gen-subset valid \\\n",
        "    --remove-bpe=sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcEIQS3uD2jl"
      },
      "source": [
        "#200 epoch - test set\n",
        "# Obtain the BLEU score for test dataset\n",
        "# For independent vocabulary, update the scoring command as, \"fairseq-generate data-bin/en_si_unigram --source-lang en --target-lang si\"\n",
        "os.chdir(\"your directory\")\n",
        "!fairseq-generate \\\n",
        "    data-bin/en_si_unigram5000/ \\\n",
        "    --source-lang en --target-lang si \\\n",
        "    --path checkpoints/checkpoint_best.pt \\\n",
        "    --beam 5 --lenpen 1.2 \\\n",
        "    --gen-subset test \\\n",
        "    --remove-bpe=sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNIUtV7j8o8l"
      },
      "source": [
        "# Obtain the checkpoint ensembled BLEU score for validation dataset\n",
        "# For independent vocabulary, update the scoring command as, \"fairseq-generate data-bin/en_si_unigram --source-lang en --target-lang si\"\n",
        "os.chdir(\"your directory\")\n",
        "!fairseq-generate \\\n",
        "    data-bin/en_si_unigram5000/ \\\n",
        "    --source-lang en --target-lang si \\\n",
        "    --path checkpoints/checkpoint_last.pt:checkpoints/checkpoint190.pt \\\n",
        "    --beam 5 --lenpen 1.2 \\\n",
        "    --gen-subset valid \\\n",
        "    --remove-bpe=sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtXTsNqq4pnD"
      },
      "source": [
        "# Obtain the checkpoint ensembled BLEU score for test dataset\n",
        "# For independent vocabulary, update the scoring command as, \"fairseq-generate data-bin/en_si_unigram --source-lang en --target-lang si\"\n",
        "os.chdir(\"your directory\")\n",
        "!fairseq-generate \\\n",
        "    data-bin/en_si_unigram5000/ \\\n",
        "    --source-lang en --target-lang si \\\n",
        "    --path checkpoints/checkpoint_last.pt:checkpoints/checkpoint190.pt \\\n",
        "    --beam 5 --lenpen 1.2 \\\n",
        "    --gen-subset test \\\n",
        "    --remove-bpe=sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djg19hFn8m_O"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkY_VbjYUaDG"
      },
      "source": [
        "# Visualize using tensorboard\n",
        "%tensorboard --logdir logs/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
